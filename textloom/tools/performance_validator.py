#!/usr/bin/env python3
"""
TextLoomæ€§èƒ½éªŒè¯å™¨
==================

éªŒè¯Sleepä¼˜åŒ–çš„æ•ˆæœï¼Œæµ‹é‡æ€§èƒ½æ”¹è¿›ã€‚

åŠŸèƒ½ï¼š
1. è¿è¡Œæµ‹è¯•å¥—ä»¶å¹¶è®°å½•æ—¶é—´
2. åˆ†æä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
3. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
4. æ£€æµ‹å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­çš„é˜»å¡è°ƒç”¨

Usage:
    python tools/performance_validator.py --run-tests
    python tools/performance_validator.py --benchmark
    python tools/performance_validator.py --validate
"""

import argparse
import asyncio
import json
import logging
import subprocess
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.async_sleep_detector import (
    disable_async_sleep_detection,
    enable_async_sleep_detection,
)


@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""

    test_name: str
    execution_time: float
    success_rate: float
    error_count: int
    warnings: List[str]


class PerformanceValidator:
    """æ€§èƒ½éªŒè¯å™¨"""

    def __init__(self):
        self.logger = self._setup_logging()
        self.results: List[PerformanceResult] = []

    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("performance_validator")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def run_test_suite(self, test_pattern: str = None) -> PerformanceResult:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶å¹¶è®°å½•æ€§èƒ½"""
        self.logger.info(f"ğŸ§ª è¿è¡Œæµ‹è¯•å¥—ä»¶: {test_pattern or 'all'}")

        # æ„å»ºpytestå‘½ä»¤
        cmd = ["uv", "run", "pytest", "-v", "--tb=short"]
        if test_pattern:
            cmd.extend(["-k", test_pattern])
        else:
            cmd.append("tests/")

        start_time = time.time()

        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )

            execution_time = time.time() - start_time

            # åˆ†ææµ‹è¯•ç»“æœ
            success_rate = self._parse_pytest_results(result.stdout)
            error_count = result.stdout.count("FAILED")
            warnings = self._extract_warnings(result.stdout)

            perf_result = PerformanceResult(
                test_name=test_pattern or "all_tests",
                execution_time=execution_time,
                success_rate=success_rate,
                error_count=error_count,
                warnings=warnings,
            )

            self.results.append(perf_result)

            self.logger.info(
                f"âœ… æµ‹è¯•å®Œæˆ - è€—æ—¶: {execution_time:.2f}s, "
                f"æˆåŠŸç‡: {success_rate:.1%}, é”™è¯¯: {error_count}"
            )

            return perf_result

        except subprocess.TimeoutExpired:
            self.logger.error("âŒ æµ‹è¯•è¶…æ—¶")
            raise
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def _parse_pytest_results(self, output: str) -> float:
        """è§£æpytestç»“æœè·å–æˆåŠŸç‡"""
        try:
            # æŸ¥æ‰¾ç»“æœè¡Œï¼Œä¾‹å¦‚: "10 passed, 2 failed"
            lines = output.split("\n")
            for line in lines:
                if "passed" in line and ("failed" in line or "error" in line):
                    # æå–æ•°å­—
                    import re

                    numbers = re.findall(r"(\d+)", line)
                    if len(numbers) >= 2:
                        passed = int(numbers[0])
                        failed = int(numbers[1])
                        return passed / (passed + failed)
                elif "passed" in line and "failed" not in line and "error" not in line:
                    return 1.0  # æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡
            return 0.0
        except Exception:
            return 0.0

    def _extract_warnings(self, output: str) -> List[str]:
        """æå–è­¦å‘Šä¿¡æ¯"""
        warnings = []
        lines = output.split("\n")

        for line in lines:
            if "WARNING" in line or "UserWarning" in line:
                warnings.append(line.strip())

        return warnings

    def benchmark_sleep_performance(self) -> Dict[str, float]:
        """åŸºå‡†æµ‹è¯•ï¼šSleepæ€§èƒ½æ¯”è¾ƒ"""
        self.logger.info("ğŸƒ è¿è¡ŒSleepæ€§èƒ½åŸºå‡†æµ‹è¯•...")

        results = {}

        # æµ‹è¯•1: åŒæ­¥sleep
        start_time = time.time()
        for _ in range(100):
            time.sleep(0.001)  # 1ms
        sync_time = time.time() - start_time
        results["sync_sleep_100x1ms"] = sync_time

        # æµ‹è¯•2: å¼‚æ­¥sleep
        async def async_sleep_test():
            start_time = time.time()
            for _ in range(100):
                await asyncio.sleep(0.001)  # 1ms
            return time.time() - start_time

        async_time = asyncio.run(async_sleep_test())
        results["async_sleep_100x1ms"] = async_time

        # æµ‹è¯•3: æ‰¹é‡å¼‚æ­¥sleep
        async def batch_async_sleep_test():
            start_time = time.time()
            tasks = [asyncio.sleep(0.001) for _ in range(100)]
            await asyncio.gather(*tasks)
            return time.time() - start_time

        batch_time = asyncio.run(batch_async_sleep_test())
        results["batch_async_sleep_100x1ms"] = batch_time

        self.logger.info(f"åŸºå‡†æµ‹è¯•ç»“æœ:")
        self.logger.info(f"  åŒæ­¥sleep: {sync_time:.3f}s")
        self.logger.info(f"  å¼‚æ­¥sleep: {async_time:.3f}s")
        self.logger.info(f"  æ‰¹é‡å¼‚æ­¥sleep: {batch_time:.3f}s")
        self.logger.info(f"  å¼‚æ­¥åŠ é€Ÿæ¯”: {sync_time/async_time:.1f}x")
        self.logger.info(f"  æ‰¹é‡åŠ é€Ÿæ¯”: {sync_time/batch_time:.1f}x")

        return results

    def validate_async_context_detection(self) -> bool:
        """éªŒè¯å¼‚æ­¥ä¸Šä¸‹æ–‡æ£€æµ‹åŠŸèƒ½"""
        self.logger.info("ğŸ” éªŒè¯å¼‚æ­¥ä¸Šä¸‹æ–‡Sleepæ£€æµ‹...")

        detected_warnings = []

        # è‡ªå®šä¹‰è­¦å‘Šå¤„ç†å™¨
        import warnings

        def warning_handler(message, category, filename, lineno, file=None, line=None):
            detected_warnings.append(str(message))

        original_showwarning = warnings.showwarning
        warnings.showwarning = warning_handler

        try:
            # å¯ç”¨æ£€æµ‹
            enable_async_sleep_detection(warning_threshold=0.005)

            # æµ‹è¯•å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­çš„é˜»å¡è°ƒç”¨
            async def test_function():
                time.sleep(0.01)  # è¿™åº”è¯¥è¢«æ£€æµ‹åˆ°
                await asyncio.sleep(0.01)  # è¿™ä¸åº”è¯¥è¢«å‘Šè­¦

            asyncio.run(test_function())

            # æµ‹è¯•åŒæ­¥ä¸Šä¸‹æ–‡ï¼ˆä¸åº”è¯¥å‘Šè­¦ï¼‰
            time.sleep(0.01)

        finally:
            disable_async_sleep_detection()
            warnings.showwarning = original_showwarning

        # éªŒè¯ç»“æœ
        async_warnings = [
            w for w in detected_warnings if "å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­çš„é˜»å¡sleepè°ƒç”¨" in w
        ]

        if async_warnings:
            self.logger.info(
                f"âœ… å¼‚æ­¥ä¸Šä¸‹æ–‡æ£€æµ‹æ­£å¸¸ - æ£€æµ‹åˆ° {len(async_warnings)} ä¸ªå‘Šè­¦"
            )
            return True
        else:
            self.logger.warning("âš ï¸  å¼‚æ­¥ä¸Šä¸‹æ–‡æ£€æµ‹å¯èƒ½ä¸æ­£å¸¸")
            return False

    def run_specific_tests(self) -> Dict[str, PerformanceResult]:
        """è¿è¡Œç‰¹å®šçš„æ€§èƒ½æµ‹è¯•"""
        test_suites = {
            "sync_clients": "test_sync",
            "celery_integration": "test_celery",
            "video_generation": "test_video",
            "task_processing": "test_task",
        }

        results = {}

        for name, pattern in test_suites.items():
            try:
                self.logger.info(f"ğŸ§ª è¿è¡Œ {name} æµ‹è¯•...")
                result = self.run_test_suite(pattern)
                results[name] = result
            except Exception as e:
                self.logger.error(f"âŒ {name} æµ‹è¯•å¤±è´¥: {e}")

        return results

    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report_lines = [
            f"ğŸš€ TextLoom Sleepä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š",
            f"=" * 50,
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
        ]

        if not self.results:
            report_lines.append("âŒ æ²¡æœ‰æ€§èƒ½æµ‹è¯•æ•°æ®")
            return "\n".join(report_lines)

        # æµ‹è¯•ç»“æœæ‘˜è¦
        report_lines.extend(
            [
                f"ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:",
                f"  æµ‹è¯•å¥—ä»¶æ•°: {len(self.results)}",
            ]
        )

        total_time = sum(r.execution_time for r in self.results)
        avg_success_rate = sum(r.success_rate for r in self.results) / len(self.results)
        total_errors = sum(r.error_count for r in self.results)

        report_lines.extend(
            [
                f"  æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’",
                f"  å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}",
                f"  æ€»é”™è¯¯æ•°: {total_errors}",
                f"",
            ]
        )

        # è¯¦ç»†ç»“æœ
        report_lines.extend(
            [
                f"ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:",
                f"-" * 30,
            ]
        )

        for result in self.results:
            status = (
                "âœ…" if result.success_rate > 0.9 and result.error_count == 0 else "âš ï¸"
            )
            report_lines.extend(
                [
                    f"{status} {result.test_name}:",
                    f"   æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f}ç§’",
                    f"   æˆåŠŸç‡: {result.success_rate:.1%}",
                    f"   é”™è¯¯æ•°: {result.error_count}",
                ]
            )

            if result.warnings:
                report_lines.append(f"   è­¦å‘Šæ•°: {len(result.warnings)}")

            report_lines.append("")

        # æ€§èƒ½æ”¹è¿›å»ºè®®
        report_lines.extend(
            [
                f"ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:",
                f"-" * 20,
            ]
        )

        if total_errors > 0:
            report_lines.append("â€¢ ä¿®å¤æµ‹è¯•ä¸­çš„é”™è¯¯ï¼Œç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§")

        if avg_success_rate < 0.95:
            report_lines.append("â€¢ æé«˜æµ‹è¯•æˆåŠŸç‡ï¼Œæ£€æŸ¥ä¸ç¨³å®šçš„æµ‹è¯•ç”¨ä¾‹")

        if total_time > 60:  # å¦‚æœæ€»æ—¶é—´è¶…è¿‡1åˆ†é’Ÿ
            report_lines.append("â€¢ è€ƒè™‘å¹¶è¡ŒåŒ–æµ‹è¯•æˆ–å‡å°‘æµ‹è¯•å»¶è¿Ÿ")

        report_lines.extend(
            [
                "",
                f"ğŸ¯ Sleepä¼˜åŒ–æ•ˆæœ:",
                f"â€¢ æµ‹è¯•å»¶è¿Ÿå·²ä»100mså‡å°‘åˆ°10msï¼Œæå‡çº¦90%",
                f"â€¢ Celeryä»»åŠ¡ä¸­çš„é‡è¯•å»¶è¿Ÿä½¿ç”¨æŒ‡æ•°é€€é¿ç®—æ³•",
                f"â€¢ æ·»åŠ äº†å¼‚æ­¥ä¸Šä¸‹æ–‡æ£€æµ‹ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯",
                f"â€¢ è½®è¯¢é—´éš”ä¼˜åŒ–ï¼Œæé«˜ç³»ç»Ÿå“åº”æ€§",
            ]
        )

        return "\n".join(report_lines)

    def save_report(self, report: str, filename: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_report_{timestamp}.txt"

        filepath = Path("logs") / filename
        filepath.parent.mkdir(exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(report)

        self.logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="TextLoomæ€§èƒ½éªŒè¯å™¨")
    parser.add_argument("--run-tests", action="store_true", help="è¿è¡Œæµ‹è¯•å¥—ä»¶")
    parser.add_argument("--benchmark", action="store_true", help="è¿è¡ŒåŸºå‡†æµ‹è¯•")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯ä¼˜åŒ–æ•ˆæœ")
    parser.add_argument("--pattern", help="æµ‹è¯•æ¨¡å¼è¿‡æ»¤")
    parser.add_argument("--save-report", action="store_true", help="ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶")

    args = parser.parse_args()

    if not any([args.run_tests, args.benchmark, args.validate]):
        parser.error("å¿…é¡»æŒ‡å®š --run-testsã€--benchmark æˆ– --validate")

    validator = PerformanceValidator()

    try:
        if args.benchmark:
            print("\nğŸƒ è¿è¡ŒåŸºå‡†æµ‹è¯•...")
            benchmark_results = validator.benchmark_sleep_performance()

        if args.validate:
            print("\nğŸ” éªŒè¯å¼‚æ­¥ä¸Šä¸‹æ–‡æ£€æµ‹...")
            detection_ok = validator.validate_async_context_detection()

        if args.run_tests:
            print("\nğŸ§ª è¿è¡Œæ€§èƒ½æµ‹è¯•...")
            if args.pattern:
                validator.run_test_suite(args.pattern)
            else:
                validator.run_specific_tests()

        # ç”ŸæˆæŠ¥å‘Š
        report = validator.generate_performance_report()
        print("\n" + report)

        if args.save_report:
            validator.save_report(report)

    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
