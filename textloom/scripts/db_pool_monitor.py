#!/usr/bin/env python3
"""
æ•°æ®åº“è¿æ¥æ± å®æ—¶ç›‘æ§å·¥å…·
ç›‘æ§è¿æ¥æ± çŠ¶æ€ï¼Œæ£€æµ‹è¿æ¥æ³„éœ²å’Œæ€§èƒ½é—®é¢˜
"""

import asyncio
import json
import logging
import os
import sys
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import settings
from models.celery_db import get_sync_connection_pool
from models.db_connection import check_connection_pool_health, get_engine

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class PoolMetrics:
    """è¿æ¥æ± æŒ‡æ ‡"""

    timestamp: str
    service_name: str
    pool_size: int
    max_overflow: int
    checked_out: int
    checked_in: int
    overflow: int
    total_connections: int
    utilization_rate: float
    is_healthy: bool
    response_time_ms: Optional[float] = None


class DatabasePoolMonitor:
    """æ•°æ®åº“è¿æ¥æ± ç›‘æ§å™¨"""

    def __init__(
        self, alert_threshold: float = 80.0, log_file: str = "logs/db_pool_monitor.log"
    ):
        self.alert_threshold = alert_threshold
        self.log_file = log_file
        self.metrics_history: List[PoolMetrics] = []
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)

        # è®¾ç½®æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(self.log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        file_handler.setFormatter(formatter)

        # æ·»åŠ åˆ°logger
        monitor_logger = logging.getLogger("db_pool_monitor")
        monitor_logger.addHandler(file_handler)
        monitor_logger.setLevel(logging.INFO)
        self.monitor_logger = monitor_logger

    async def collect_async_pool_metrics(self) -> PoolMetrics:
        """æ”¶é›†å¼‚æ­¥è¿æ¥æ± æŒ‡æ ‡"""
        start_time = time.time()

        try:
            pool_health = await check_connection_pool_health()
            response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

            pool_size = pool_health.get("pool_size", 0)
            checked_out = pool_health.get("checked_out", 0)
            checked_in = pool_health.get("checked_in", 0)
            overflow = pool_health.get("overflow", 0)

            total_connections = checked_out + overflow
            utilization_rate = (
                total_connections / max(pool_size + settings.database_max_overflow, 1)
            ) * 100

            metrics = PoolMetrics(
                timestamp=datetime.now().isoformat(),
                service_name="FastAPI_Async",
                pool_size=pool_size,
                max_overflow=settings.database_max_overflow,
                checked_out=checked_out,
                checked_in=checked_in,
                overflow=overflow,
                total_connections=total_connections,
                utilization_rate=utilization_rate,
                is_healthy=pool_health.get("is_healthy", False),
                response_time_ms=response_time,
            )

            # è®°å½•æŒ‡æ ‡
            self.monitor_logger.info(
                f"AsyncPool Metrics: {json.dumps(asdict(metrics))}"
            )

            # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            if utilization_rate > self.alert_threshold:
                self._trigger_alert("HIGH_UTILIZATION", metrics)

            if not metrics.is_healthy:
                self._trigger_alert("HEALTH_CHECK_FAILED", metrics)

            if response_time > 5000:  # 5ç§’
                self._trigger_alert("SLOW_RESPONSE", metrics)

            return metrics

        except Exception as e:
            logger.error(f"æ”¶é›†å¼‚æ­¥è¿æ¥æ± æŒ‡æ ‡å¤±è´¥: {e}")
            return PoolMetrics(
                timestamp=datetime.now().isoformat(),
                service_name="FastAPI_Async",
                pool_size=0,
                max_overflow=0,
                checked_out=0,
                checked_in=0,
                overflow=0,
                total_connections=0,
                utilization_rate=0,
                is_healthy=False,
                response_time_ms=(time.time() - start_time) * 1000,
            )

    def collect_sync_pool_metrics(self) -> PoolMetrics:
        """æ”¶é›†åŒæ­¥è¿æ¥æ± æŒ‡æ ‡"""
        start_time = time.time()

        try:
            # å°è¯•è·å–è¿æ¥æ± çŠ¶æ€
            pool = get_sync_connection_pool()
            response_time = (time.time() - start_time) * 1000

            # ThreadedConnectionPoolæ²¡æœ‰è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
            # ä½¿ç”¨é…ç½®å€¼ä½œä¸ºåŸºå‡†
            pool_size = settings.celery_database_pool_size
            max_overflow = settings.celery_database_max_overflow

            # ä¼°ç®—è¿æ¥ä½¿ç”¨æƒ…å†µ(åŸºäºé…ç½®)
            estimated_utilization = 50.0  # ç”±äºæ— æ³•ç›´æ¥è·å–ï¼Œä½¿ç”¨ä¼°ç®—å€¼

            metrics = PoolMetrics(
                timestamp=datetime.now().isoformat(),
                service_name="Celery_Sync",
                pool_size=pool_size,
                max_overflow=max_overflow,
                checked_out=0,  # ThreadedConnectionPoolæ— æ³•ç›´æ¥è·å–
                checked_in=0,  # ThreadedConnectionPoolæ— æ³•ç›´æ¥è·å–
                overflow=0,  # ThreadedConnectionPoolæ— æ³•ç›´æ¥è·å–
                total_connections=pool_size,  # ä¼°ç®—å€¼
                utilization_rate=estimated_utilization,
                is_healthy=True,  # å¦‚æœèƒ½è·å–åˆ°poolå¯¹è±¡ï¼Œè®¤ä¸ºæ˜¯å¥åº·çš„
                response_time_ms=response_time,
            )

            # è®°å½•æŒ‡æ ‡
            self.monitor_logger.info(f"SyncPool Metrics: {json.dumps(asdict(metrics))}")

            return metrics

        except Exception as e:
            logger.error(f"æ”¶é›†åŒæ­¥è¿æ¥æ± æŒ‡æ ‡å¤±è´¥: {e}")
            return PoolMetrics(
                timestamp=datetime.now().isoformat(),
                service_name="Celery_Sync",
                pool_size=0,
                max_overflow=0,
                checked_out=0,
                checked_in=0,
                overflow=0,
                total_connections=0,
                utilization_rate=0,
                is_healthy=False,
                response_time_ms=(time.time() - start_time) * 1000,
            )

    def _trigger_alert(self, alert_type: str, metrics: PoolMetrics):
        """è§¦å‘å‘Šè­¦"""
        alert_message = {
            "alert_type": alert_type,
            "service": metrics.service_name,
            "timestamp": metrics.timestamp,
            "utilization_rate": metrics.utilization_rate,
            "threshold": self.alert_threshold,
            "details": asdict(metrics),
        }

        self.monitor_logger.warning(f"ALERT {alert_type}: {json.dumps(alert_message)}")
        print(
            f"ğŸš¨ ALERT [{alert_type}] {metrics.service_name}: åˆ©ç”¨ç‡ {metrics.utilization_rate:.1f}%"
        )

    async def run_monitoring_cycle(self) -> List[PoolMetrics]:
        """è¿è¡Œä¸€æ¬¡ç›‘æ§å‘¨æœŸ"""
        cycle_metrics = []

        # æ”¶é›†å¼‚æ­¥è¿æ¥æ± æŒ‡æ ‡
        async_metrics = await self.collect_async_pool_metrics()
        cycle_metrics.append(async_metrics)

        # æ”¶é›†åŒæ­¥è¿æ¥æ± æŒ‡æ ‡
        sync_metrics = self.collect_sync_pool_metrics()
        cycle_metrics.append(sync_metrics)

        # æ·»åŠ åˆ°å†å²è®°å½•
        self.metrics_history.extend(cycle_metrics)

        # ä¿æŒå†å²è®°å½•å¤§å°(æœ€å¤šä¿ç•™1000æ¡)
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-1000:]

        return cycle_metrics

    def generate_summary_report(self, hours: int = 1) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        # è¿‡æ»¤æœ€è¿‘çš„æŒ‡æ ‡
        recent_metrics = [
            m
            for m in self.metrics_history
            if datetime.fromisoformat(m.timestamp) > cutoff_time
        ]

        if not recent_metrics:
            return {"message": "æ²¡æœ‰æœ€è¿‘çš„æŒ‡æ ‡æ•°æ®"}

        # æŒ‰æœåŠ¡åˆ†ç»„
        services = {}
        for metric in recent_metrics:
            service = metric.service_name
            if service not in services:
                services[service] = []
            services[service].append(metric)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = {
            "time_range": f"æœ€è¿‘{hours}å°æ—¶",
            "total_samples": len(recent_metrics),
            "services": {},
        }

        for service_name, metrics in services.items():
            if not metrics:
                continue

            utilization_rates = [
                m.utilization_rate for m in metrics if m.utilization_rate is not None
            ]
            response_times = [
                m.response_time_ms for m in metrics if m.response_time_ms is not None
            ]
            health_checks = [m.is_healthy for m in metrics]

            service_summary = {
                "sample_count": len(metrics),
                "avg_utilization": (
                    sum(utilization_rates) / len(utilization_rates)
                    if utilization_rates
                    else 0
                ),
                "max_utilization": max(utilization_rates) if utilization_rates else 0,
                "avg_response_time_ms": (
                    sum(response_times) / len(response_times) if response_times else 0
                ),
                "max_response_time_ms": max(response_times) if response_times else 0,
                "health_check_success_rate": (
                    sum(health_checks) / len(health_checks) * 100
                    if health_checks
                    else 0
                ),
                "alerts_count": sum(
                    1 for m in metrics if m.utilization_rate > self.alert_threshold
                ),
            }

            summary["services"][service_name] = service_summary

        return summary

    def print_current_status(self, metrics: List[PoolMetrics]):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        print(f"\nğŸ“Š æ•°æ®åº“è¿æ¥æ± çŠ¶æ€ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

        for metric in metrics:
            status_icon = "âœ…" if metric.is_healthy else "âŒ"
            utilization_icon = (
                "ğŸ”´" if metric.utilization_rate > self.alert_threshold else "ğŸŸ¢"
            )

            print(f"{status_icon} {metric.service_name}")
            print(
                f"   è¿æ¥æ± å¤§å°: {metric.pool_size} (æœ€å¤§æº¢å‡º: {metric.max_overflow})"
            )
            print(f"   å½“å‰ä½¿ç”¨: {metric.checked_out} è¿æ¥")
            print(f"   æº¢å‡ºè¿æ¥: {metric.overflow}")
            print(f"   {utilization_icon} åˆ©ç”¨ç‡: {metric.utilization_rate:.1f}%")
            if metric.response_time_ms is not None:
                print(f"   å“åº”æ—¶é—´: {metric.response_time_ms:.1f}ms")
            print()


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ•°æ®åº“è¿æ¥æ± ç›‘æ§å·¥å…·")
    parser.add_argument("--interval", type=int, default=30, help="ç›‘æ§é—´éš”(ç§’)")
    parser.add_argument(
        "--duration", type=int, default=0, help="ç›‘æ§æ—¶é•¿(ç§’ï¼Œ0è¡¨ç¤ºæŒç»­ç›‘æ§)"
    )
    parser.add_argument(
        "--alert-threshold", type=float, default=80.0, help="å‘Šè­¦é˜ˆå€¼(ç™¾åˆ†æ¯”)"
    )
    parser.add_argument(
        "--report-interval", type=int, default=300, help="æŠ¥å‘Šç”Ÿæˆé—´éš”(ç§’)"
    )
    parser.add_argument("--one-shot", action="store_true", help="åªè¿è¡Œä¸€æ¬¡æ£€æŸ¥")

    args = parser.parse_args()

    monitor = DatabasePoolMonitor(alert_threshold=args.alert_threshold)

    print("ğŸš€ å¯åŠ¨æ•°æ®åº“è¿æ¥æ± ç›‘æ§...")
    print(f"â±ï¸  ç›‘æ§é—´éš”: {args.interval}ç§’")
    print(f"ğŸš¨ å‘Šè­¦é˜ˆå€¼: {args.alert_threshold}%")
    print(f"ğŸ“Š æŠ¥å‘Šé—´éš”: {args.report_interval}ç§’")

    if args.one_shot:
        print("\nğŸ” è¿è¡Œå•æ¬¡æ£€æŸ¥...")
        metrics = await monitor.run_monitoring_cycle()
        monitor.print_current_status(metrics)
        return

    start_time = time.time()
    last_report_time = start_time

    try:
        while True:
            current_time = time.time()

            # è¿è¡Œç›‘æ§å‘¨æœŸ
            metrics = await monitor.run_monitoring_cycle()
            monitor.print_current_status(metrics)

            # å®šæœŸç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            if current_time - last_report_time >= args.report_interval:
                print("\nğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
                summary = monitor.generate_summary_report()
                print(json.dumps(summary, indent=2, ensure_ascii=False))
                last_report_time = current_time

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if args.duration > 0 and (current_time - start_time) >= args.duration:
                print(f"\nâ° ç›‘æ§æ—¶é•¿å·²è¾¾åˆ° {args.duration} ç§’ï¼Œåœæ­¢ç›‘æ§")
                break

            # ç­‰å¾…ä¸‹ä¸€ä¸ªç›‘æ§å‘¨æœŸ
            await asyncio.sleep(args.interval)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ç›‘æ§")
    except Exception as e:
        print(f"\nâŒ ç›‘æ§è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logger.exception("ç›‘æ§å¤±è´¥")
    finally:
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\nğŸ“‹ ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š...")
        final_summary = monitor.generate_summary_report(hours=24)
        print(json.dumps(final_summary, indent=2, ensure_ascii=False))

        print("\nâœ… ç›‘æ§ç»“æŸ")
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {monitor.log_file}")


if __name__ == "__main__":
    asyncio.run(main())
